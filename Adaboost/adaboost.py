import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier

class Adaboost:
    
    # Class to perform the Adaboost boosting technique
    
    def __init__(self, n_estimators=10):
        
        # constructor: parameters: n_estimators: Number of estimators for the boosting technique  
        
        self.n_estimators=n_estimators
        
        # Initializing the Decision Stumps
        self.dcts = [ DecisionTreeClassifier(max_depth=1) for i in range(n_estimators)]
        
        # Initializing the weights of all the Decision Stumps
        self.alphas=[0 for i in range(n_estimators)]
        
        # Storing the Unique labels that are encountered
        self.labels_encountered=[]
    
    def not_match(self,y1,y2):
        
        # function to return the indices for which the elements of the two arrays y1 and y2 are not same
        # parameters: 
        # y1: numpy array
        # y2: numpy array
        
        # Returns:  the indices for which the elements of the two arrays y1 and y2 are not same
        
        indices=[]
        for i in range(len(y1)):
            if y1[i]!=y2[i]:
                indices.append(i)
        return indices
    
    def return_bin(self,weight):
        
        # function to generate random numbers and return the index of the bin(of weights) that the random number falls in 
        
        random=np.random.random()

        sum1=0
        for i in range(len(weight)):
            if (random>sum1) & (random<=weight[i]+sum1):
                return i
            sum1=weight[i]+sum1
    
    def boost(self,X_train,y_train,dct):
        
        # function to perform the boosting technique.
        
        # Params:
        # X_train: features of the training set
        # y_train: labels of the training set
        # dct: estimator to perform training 
        
        
        len_data=len(X_train)
        
        # initializing the weight array
        weights= np.array([ 1/len_data for i in range(len(X_train)) ])
        
        # Training the estimator: The Decision Stump will pick up the best column while training 
        dct.fit(X_train,y_train)
        
        # Predicting the training set
        y_pred=dct.predict(X_train)
        
        # Calculating the error based on the predicted values 
        error=weights[self.not_match(y_pred,y_train.values)].sum()
        
        # Calculating the alpha from error 
        if error == 0:
            alpha= 1/2 * np.log((1-0.001)/(0.001))
        else:
            alpha= 1/2 * np.log((1-error)/(error))

        # Calculating new weights based on the predicted values
        for i in range(len_data):
            if y_pred[i]==y_train.values[i]:
                weights[i]= 1/len_data * np.exp(alpha)
            else:
                weights[i]= 1/len_data * np.exp(-alpha)
                
        # Normalizing the weights
        weights=weights/weights.sum()
        
        # Initializing the new dataset
        new_dataset=pd.DataFrame(columns=X_train.columns)
        new_y=pd.Series(name=y_train.name)

        # Creating the new dataset based on the indices returned by the "return_bin" function
        for i in range(len_data):
            index=self.return_bin(weights)
            new_dataset = new_dataset.append(dict(zip(X_train.columns,X_train.values[index] )), ignore_index=True)
            new_y=new_y.append(pd.Series(y_train.values[index]),ignore_index=True)
            
        return alpha, new_dataset,new_y,dct
    
    def fit(self,X_train,y_train):
        
        # function to train on the training data
        
        # Making a copy of the training data to work with
        train=X_train.copy()
        labels=y_train.copy()
        
        # Storing the unique labels encountered
        self.labels_encountered=labels.unique()
        
        # Calculating the decision stumps and weights for the decision stump
        for i in range(self.n_estimators):
            alpha,train,labels,dct=self.boost(train,labels,self.dcts[i])
            self.alphas[i]=alpha
            self.dcts[i]=dct
    
    def predict(self,test):
        
        labels=[]
        for i in range(len(test)):
            
            # Initializing a dictionary to store the total weight of the label generated by an estimator
            prediction=dict()
            for j in self.labels_encountered:
                prediction[j]=0
                
            # Calculating the total weights of the labels predicted by the decision stumps 
            for j in range(self.n_estimators):
                pred=self.dcts[j].predict([test.iloc[i]])[0]
                prediction[pred]+=self.alphas[j]
            
            # Getting the label with maximum weight
            labels.append(max(prediction,key=prediction.get))
        return labels